{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end-non-blind-deconvolution\n",
    "\n",
    "This notebook builds an end-to-end non-blind-deconvolution train rgd optimizer using and torch==1.10 \n",
    "\n",
    "## 1. Problem\n",
    "\n",
    "Non blind deconvolution\n",
    "we have blure image and the kernel that has been used to be blured, we need to get the laten image \n",
    "y = k ∗ x + n\n",
    "\n",
    "* y ∈ R^m : blurred image \n",
    "* k ∈ R^l : corresponding blur kernel k\n",
    "* x ∈ R^n : latent image  \n",
    "* ∗ denotes the convolution operator\n",
    "* n ∈ R^m : denotes an i.i.d. white Gaussian noise term with unknown standard deviation (i.e. noise level).\n",
    "\n",
    "givin y and k, we need to estimate x\n",
    "\n",
    "k is the psf of an eye of individuals having refractive vision problems. !\n",
    "\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "The data we're using till now: *DATASET FROM HRTR: http://chaladze.com/l5/* \n",
    "//could be updated later\n",
    "\n",
    "\n",
    "## 3. Evaluation\n",
    "\n",
    "The evaluation is **PSNR** and **SSIM** //not commited yet\n",
    "Now is also visualisation of output photos VS originl photos 'ground truth'\n",
    "\n",
    "\n",
    "## 4. Features\n",
    "\n",
    "Some information about the data:\n",
    "* We're dealing with images (unstructured data) so it's probably best we use deep learning/transfer learning.\n",
    "* We're generating dataset : blur kernel k, and n noise , and then y by ourself   \n",
    "* \n",
    "* . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for dataset folder:\n",
    "DATA_DIR = \"/home/jovyan/Sarasweet/rgdn-optimizer-based-deconvolution/dataFiles/Linnaeus 5 256X256/\"\n",
    "\n",
    "#num_params for psf - fpr sweet project that makes psf \n",
    "num_params=5 # or 20 later! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our workspace ready\n",
    "\n",
    "* Reload all modules (except those excluded by %aimport) automatically now. '%autoreload'\n",
    "* Reload all modules (except those excluded by %aimport) every time before executing the Python code typed. '%autoreload 2' ✅\n",
    "* Import torch=1.10 ✅\n",
    "* Make sure we're using a GPU ✅\n",
    "* install lib  pydoe, torchsummary, torchmetrics, piq ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install pydoe -q\n",
    "# !pip install torchsummary -q\n",
    "# !pip install torchmetrics -q\n",
    "# !pip install piq -q\n",
    "\n",
    "\n",
    "# !pip install torch==1.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.10.0+cu102\n",
      "GPU not available :(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary tools\n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"TF version:\", torch.__version__)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"GPU\", \"available (YESSSS!!!!!)\" if torch.cuda.is_available() else \"not available :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our data ready (turning into Tensors)\n",
    "\n",
    "With all machine learning models, our data has to be in numerical format. So that's what we'll be doing first. Turning our images into Tensors (numerical representations).\n",
    "\n",
    "Let's start by accessing our data and checking out the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make our dataset\n",
    "\n",
    "# from __future__ import print_function, absolute_import\n",
    "import torch\n",
    "import os\n",
    "# import scipy.io as sio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "# from data.make_kernel import kernel_sim_spline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import image\n",
    "from pathlib import Path\n",
    "# from operator import itemgetter\n",
    "from skimage.color import rgb2gray\n",
    "import pyDOE as doe # LHS\n",
    "from scipy.stats.distributions import uniform\n",
    "from tqdm import tqdm\n",
    "# from skimage import util, filters\n",
    "# from scipy import signal\n",
    "\n",
    "from sweet.sweet import Sweet\n",
    "from sweet.util.fft import shift, fft_conv\n",
    "\n",
    "\n",
    "fixed_params = {\n",
    "    'pupil_diam' : 2.5,\n",
    "    'A' : 0\n",
    "}\n",
    "\n",
    "class BlurryImageDatasetOnTheFlySweet(Dataset):\n",
    "    '''\n",
    "    Implemented to use an eye's PSF from Sweet.\n",
    "    PSF is not cropped.\n",
    "    Convolution is perfomed with fft_conv.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 file_name_list,\n",
    "                 is_rgb=True,\n",
    "                 k_size=256,\n",
    "                 patch_size=256,\n",
    "                 max_num_images=None):\n",
    "        assert is_rgb == False, 'RGB images currently not supported'\n",
    "        \n",
    "        self.file_name_list = sorted(file_name_list)\n",
    "        self.is_rgb = is_rgb\n",
    "        self.k_size = k_size\n",
    "        self.patch_size = 256\n",
    "        self.params = []\n",
    "        self.num_params = 0\n",
    "\n",
    "        if max_num_images is not None and max_num_images < len(\n",
    "                self.file_name_list):\n",
    "            self.file_name_list = self.file_name_list[:max_num_images]\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_psf(S, A, C, pupil_diam):\n",
    "        sweet = Sweet()\n",
    "        sweet.set_eye_prescription(\n",
    "            S = S,  \n",
    "            A = A,\n",
    "            C = C,  \n",
    "        )\n",
    "        sweet.set_experiment_params(\n",
    "            pupil_diam = pupil_diam,  \n",
    "            view_dist = 100.0,      \n",
    "            canvas_size_h = 10.0,  \n",
    "        )\n",
    "        psf = sweet._psf()\n",
    "        return psf\n",
    "    \n",
    "    \n",
    "    def params_init(self, params):\n",
    "        self.params = params\n",
    "        self.num_params = len(params)\n",
    "      \n",
    "        \n",
    "    def params_random_init(self, num_params): #LHS      \n",
    "        params = doe.lhs(4, samples=num_params) # order: S, A, C, pupil_diam\n",
    "        loc =   [-8, fixed_params['A'], -3, fixed_params['pupil_diam']]\n",
    "        scale = [4,  fixed_params['A'],  3, fixed_params['pupil_diam']]\n",
    "        for j in range(4):\n",
    "            if j == 1:\n",
    "                params[:, j] = fixed_params['A']\n",
    "            elif j == 3:\n",
    "                params[:, j] = fixed_params['pupil_diam']\n",
    "            else:\n",
    "                params[:, j] = uniform(loc=loc[j], scale=scale[j]).ppf(params[:, j])\n",
    "        params = params.tolist()\n",
    "        for i in tqdm(range(num_params)):\n",
    "            psf = BlurryImageDatasetOnTheFlySweet.get_psf(*params[i])\n",
    "            psf = psf / psf.sum()        ###### ADD NORMALIZING\n",
    "            params[i].append(psf)   \n",
    "        self.params = params\n",
    "        self.num_params = num_params\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_name_list)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = np.random.choice(np.arange(self.num_params))\n",
    "        k = self.params[i][-1]\n",
    "        img_name = self.file_name_list[idx]\n",
    "        sample = plt.imread(img_name)\n",
    "\n",
    "        if sample.shape[0] < self.patch_size or sample.shape[\n",
    "                1] < self.patch_size:\n",
    "            return self.__getitem__((idx - 1) % (self.__len__()))\n",
    "        patches = image.extract_patches_2d(sample,\n",
    "                                           [self.patch_size, self.patch_size],\n",
    "                                           max_patches=1)\n",
    "        sample = patches[0, ...]\n",
    "        sample = sample.astype(np.float32) / 255.0\n",
    "        if not self.is_rgb:\n",
    "            sample = rgb2gray(sample)\n",
    "        y = fft_conv(sample, k)\n",
    "        y = torch.tensor(y, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "#         #adding noise nl\n",
    "#         nl = np.random.uniform(0.003, 0.015)  #0.3% - 1.5%\n",
    "#         y = y + nl * torch.randn_like(y)\n",
    "        \n",
    "        x_gt = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)\n",
    "        k = torch.tensor(k, dtype=torch.float32).unsqueeze(0)\n",
    "        kt = torch.flip(k, [1, 2])\n",
    "        return y, x_gt, k, kt     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have:\n",
      "Train files: 6000\n",
      "Test files:2000\n"
     ]
    }
   ],
   "source": [
    "# Checkout our data\n",
    "#How many images we have \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "#path for dataset folder:\n",
    "DATA_DIR = \"/home/jovyan/Sarasweet/rgdn-optimizer-based-deconvolution/dataFiles/Linnaeus 5 256X256/\"\n",
    "\n",
    "\n",
    "#if train & test files in diffrent folders\n",
    "train_dir = Path(os.path.join(DATA_DIR, 'train'))\n",
    "test_dir = Path(os.path.join(DATA_DIR, 'test'))\n",
    "\n",
    "train_files = list(train_dir.rglob('*.jpg'))\n",
    "test_files = list(test_dir.rglob('*.jpg'))\n",
    "\n",
    "print('We have:\\nTrain files: {}\\nTest files:{}'.format(len(train_files), len(test_files)))\n",
    "\n",
    "\n",
    "# #if whole dataset (train+test) in same path folders\n",
    "# data_path=Path(DATA_DIR)\n",
    "# data_files = list(data_path.rglob('*.jpg'))\n",
    "# print('We have dataset of {} files, that need to be splited into train and test.\\n'.format(len(data_files)))\n",
    "\n",
    "\n",
    "#train_files[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800, 200, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and testing (80/20 is indeed a good starting point) but Now it's 75/25 as the files\n",
    "\n",
    "\n",
    "# Set number of images to use for experimenting\n",
    "NUM_IMAGES = 1000 #@param {type:\"slider\", min:1000, max:10000, step:1000}\n",
    "#NUM_IMAGES=len(train_files) #all images\n",
    "\n",
    "# Let's split our data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=train_files\n",
    "y=[0] * len(train_files) # just to fit the train_test_split function, in our project we have diffrent y (output)\n",
    "\n",
    "\n",
    "# Split the training data into training and validation (again, 80/20 is a fair split)   \n",
    "# Split them into training and validation of total size NUM_IMAGES\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],\n",
    "                                                  y[:NUM_IMAGES],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "len(X_train), len(y_train), len(X_val), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataset into batches\n",
    "#load data into batches, as aricle will do in batch of size(4) ! \n",
    "#article: https://arxiv.org/pdf/1804.03368.pdf\n",
    "\n",
    "def get_dataloaders(data_files: list,\n",
    "                    dataset: torch.utils.data.Dataset,\n",
    "                    num_params: int,\n",
    "                    params_filename: str,\n",
    "                    batch_size:int =4,\n",
    "                    shuffle:bool=True,\n",
    "                    drop_last:bool=False) -> (torch.utils.data.DataLoader, torch.utils.data.DataLoader):\n",
    "    \"\"\"\n",
    "    Function to get dataloaders (works only with Sweet dataset).\n",
    "    \n",
    "    :param: data_files: list with paths to data\n",
    "    :param: dataset: object of Dataset class\n",
    "    :param: num_params: number of eye parameters generated on each epoch\n",
    "    :param: params_filename: name of file to save params generated during the training\n",
    "    :param: batch_size: nefers to the number of training samples used in one iteration\n",
    "    :param: shuffle: If shuffle is set to True, then all the samples are shuffled and loaded in batches. Otherwise they are sent one-by-one without any shuffling.\n",
    "    :param: drop_last: drop_last\n",
    "\n",
    "    \n",
    "    :return: dataloder\n",
    "    \"\"\"\n",
    "    files_dataset = dataset(file_name_list=X_train,\n",
    "                            is_rgb=False,\n",
    "                            k_size=256,\n",
    "                            max_num_images=None)\n",
    "\n",
    "\n",
    "    files_dataset.params_init(train_dataset.params)  \n",
    "    \n",
    "    data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    # save_params(params_filename, files_dataset)\n",
    "    \n",
    "    return data_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d398bcf2dfe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#load data into batches, as aricle will do in batch of size(4) ! \n",
    "#article: https://arxiv.org/pdf/1804.03368.pdf\n",
    "    \n",
    "train_loader = get_dataloaders(data_files=X_train,\n",
    "                               dataset=BlurryImageDatasetOnTheFlySweet,\n",
    "                               num_params=num_params,\n",
    "                               batch_size=4,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False,\n",
    "                            params_filename=\"file_to_save_parm\")\n",
    "valid_loader = get_dataloaders(data_files=X_val,\n",
    "                               dataset=BlurryImageDatasetOnTheFlySweet,\n",
    "                               num_params=num_params,\n",
    "                               batch_size=4,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False,\n",
    "                            params_filename=\"file_to_save_parm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:00<00:08, 23.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:00<00:08, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256]) torch.Size([4, 1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "# y, x_gt, k, kt = next(iter(train_loader))\n",
    "# kj = shift(k[0].squeeze(0))\n",
    "\n",
    "# plt.imshow(kj)\n",
    "\n",
    "for y, x_gt, k, kt in tqdm(train_loader):\n",
    "    plt.imshow(k[0].squeeze(0).cpu().numpy())\n",
    "    print(np.shape(y),np.shape(x_gt))\n",
    "#     kj = k[j].squeeze(0).cpu().numpy()\n",
    "    i=i+1\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get dataset ready!\n",
    "\n",
    "#num_params for psf - fpr sweet project that makes psf \n",
    "num_params=5 # or 20 later! \n",
    "\n",
    "train_dataset = BlurryImageDatasetOnTheFlySweet(file_name_list=X_train,\n",
    "                            is_rgb=False,\n",
    "                            k_size=256,\n",
    "                            max_num_images=None)\n",
    "\n",
    "train_dataset.params_random_init(num_params)\n",
    "\n",
    "#load data into batches, as aricle will do in batch of size(4) ! \n",
    "#article: https://arxiv.org/pdf/1804.03368.pdf\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "# save_params(params_filename, files_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-10-33762a735c88>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-33762a735c88>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    valid_files = list(itemgetter(*ids[:N])(test_files))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Set number of images to use for experimenting\n",
    "NUM_IMAGES = 1000 #@param {type:\"slider\", min:1000, max:10000, step:1000}\n",
    "\n",
    "# Let's split our data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#olya's way to shuffle again  \n",
    "ids = np.random.permutation(len(test_files))\n",
    "N = len(test_files) // 2\n",
    "\n",
    "    valid_files = list(itemgetter(*ids[:N])(test_files))\n",
    "    test_files = list(itemgetter(*ids[N:])(test_files))\n",
    "\n",
    "# Split them into training and validation of total size NUM_IMAGES\n",
    "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],\n",
    "                                                  y[:NUM_IMAGES],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "len(X_train), len(y_train), len(X_val), len(y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
